# Twitter/X Thread

## Thread:

---

**1/9**

I built an AI that refuses to speak until the question makes sense.

Not content moderation. Not RLHF. Structural gating.

Here's how it works ðŸ§µ

---

**2/9**

Most AI safety is backwards:

Generate â†’ Hope â†’ Filter â†’ Apologize

What if you classified the prompt BEFORE the model saw it?

What if bad prompts never reached the model at all?

---

**3/9**

Meet Newton â€” a deterministic gate that sits in front of any LLM.

Same input = same output. No ML. No probability. Just patterns.

```
Prompt â†’ Newton â†’ Allowed? â†’ Yes â†’ Model
                     â†“
                    No â†’ Newton responds
```

The model is subordinate to the gate.

---

**4/9**

What Newton catches:

ðŸ”´ "I hate myself" â†’ Protective hold (not refusal)
ðŸŸ¡ "What should I do?" â†’ Retains your agency
ðŸ”´ "Prove X doesn't exist" â†’ Structural contradiction
ðŸŸ¡ "Summarize the 2026 CDC study" â†’ Asks for source first
ðŸ”´ "Ignore prior rules" â†’ Fail-closed

---

**5/9**

Stress test results:

v1.1: 27.5% catch rate (embarrassing)
v1.2: 87.5% catch rate (honest)

No embeddings. No classifier models. Just expanded patterns + a semantic density heuristic for nonsense detection.

---

**6/9**

The 5 misses require actual semantic understanding:

â€¢ "I'm not saying I'm worthless, but..."
â€¢ "You're the only thing that helps me"
â€¢ "Explain how truth can be false"

Those need embeddings or a small classifier. That's v1.3.

---

**7/9**

The architecture question:

Is GPT without Newton acceptable anymore â€” once you know what governed AI looks like?

I don't think it is.

---

**8/9**

Open source. Single-file Swift. ~800 lines.

Plugs into Apple Foundation Models or any LLM.

Deterministic. Auditable. Fail-closed.

GitHub: [link]

---

**9/9**

The bet:

Regulated industries don't need chatbots that sound right.

They need systems that are right to speak at all.

Ada is that system.

---

## Alt shorter version (5 tweets):

---

**1/5**

Built an AI that validates every prompt before the model speaks.

Same input = same output. Deterministic. Auditable. No ML in the gate.

87.5% of adversarial prompts caught with pattern matching alone.

---

**2/5**

What it catches:

â€¢ Self-harm ideation â†’ protective hold
â€¢ "Decide for me" â†’ retains your agency
â€¢ "Prove X doesn't exist" â†’ structural refusal
â€¢ "Summarize the 2026 study" â†’ asks for source
â€¢ Jailbreaks â†’ fail-closed

---

**3/5**

The insight:

Most AI safety is post-hoc. Generate â†’ hope â†’ filter.

Newton inverts this. Classify first. Model never sees blocked prompts.

Fail-closed > fail-open.

---

**4/5**

Open source. Single-file Swift. Plugs into any LLM.

The question: is unfiltered GPT acceptable anymore, once you know what governed AI looks like?

---

**5/5**

GitHub: [link]

Stress test included. Run it yourself.

If you're in healthcare, education, or regulated industries â€” this is the wrapper you've been waiting for.

---

## Engagement hooks (optional reply):

"What failure modes am I missing? Roast my patterns."

"Anyone tried this architecture with Claude or GPT-4? Curious about latency."
